This
  https://docs.vagrantup.com/v2/provisioning/salt.html
explains how to set up Vagrant to do a masterless salt provision.

Under the vagrant root directory (the one containing the Vagrantfile)
there is a salt/minion file containing only the line 
  file_client: local

I'm using 22.27.82. salt.states.pkgrepo to add the Ice repository for
Ubuntu to the host. See: 
  http://docs.saltstack.com/en/latest/ref/states/all/salt.states.pkgrepo.html
NOTE: you also need to add '- fromrepo: trusty-zeroc' to the pkg.installs 
(I believe).

Details on how to add the apt package repo came from
  http://www.zeroc.com/download.html
specifically, the section on Ubuntu 14.04 (Trusty Tahr).
See:
  http://download.zeroc.com/Ice/3.5/ubuntu/dists/trusty-zeroc/Contents-amd64
for a listing of what's in trusty-zeroc. Don't trust what ubuntu
suggests: ice35-services was not the right way to get icegridadmin.

--

Have set up a multi-machine Vagrantfile for icebox-*. See
  https://docs.vagrantup.com/v2/multi-machine/index.html

To run the salt provisioner on the vm itself:
  sudo salt-call --local state.highstate
To run it via Vagrant:
  vagrant provision icebox-<n>

To get IceGridGUI-3.5.1.jar, install icegrid somewhere
(e.g. provision one of the vagrant boxes) and copy it from
/usr/share/java. Run it as java -jar IceGridGUI-3.5.1.jar&.

The DATA_ROOT will depend on the environment. For a vagrant
development image it will be /home/vagrant/data. For production
it might be /home/icecap/data (or just /icecap/data?).

The APP_ROOT will be /vagrant for a vagrant dev image. In order to
make the slice files importable, APP_ROOT/python will have to be added
to the PYTHONPATH. It's ugly but slice2py does not seem to make it
easy to do any other way.

--

To run the demo:

  python /vagrant/scripts/demo_client.py

--

To run the registry admin tool:

  icegridadmin --Ice.Config=/vagrant/client.cfg

--

We use Vagrant to create VMs but we only go as far as
installing the salt minion.

It seems that the salt-master has a slightly annoying restriction that
it runs off a single /srv/salt. I suppose a web server has to have a
root somewhere. What I don't like is that after providing all the sls
files I will also need to document how to install them where the
salt-master can use them.

The bootstrapping process for salt minions is documented here:
  http://docs.saltstack.com/en/latest/topics/installation/ubuntu.html
It amounts to:
  sudo add-apt-repository -y ppa:saltstack/salt
  sudo apt-get update
  sudo apt-get -y install salt-minion

Then to point it at the master:
  sudo bash
  # echo '<salt-master-ip> salt' >> /etc/hosts
Alternatively we can edit /etc/salt/minion updating the line
  #master: salt
That also starts the minion. 

That forms part of the provisioning process for a new Vagrant box to
serve as an empty salt minion in the boxes directory. From that point
on the process of provisioning boxes is the normal salt master/minion
communication.

The master (whose installation process is similar, substituting 'sudo
apt-get -y install salt-master') will be contacted by the minion.
We check for the contact and accept the key proposed by the minion:
  sudo salt-key -L # lists any new keys as proposed
  sudo salt-key -a <minion-name>

Not clear to me why these commands require root access.
We can fix that:
  http://docs.saltstack.com/en/latest/ref/configuration/nonroot.html
Edited 'user' in /etc/salt/master and restarted salt-master.
(Not really sure if this is good though because of the warning in the
documentation about pam authentication issues.)
Page also says to do this (though config seems to suggest not needed):
  sudo chown -R oag /etc/salt /var/cache/salt /var/log/salt /var/run/salt
and then restart the salt-master again.

Got some "Data failed to compile === No matching sls" errors.
Went back to root again during efforts to fix all the "Data failed.."
problems. 

It seems the reason for this problem was a mismatch between the salt
versions installed on the master and minion (with the minion being
ahead due to my use of the repo there but not on the master).
I repeated the above apt-add-repository on the master and did
  sudo apt-get install salt-master --upgrade.
After that both master and client report the same version.

Symlinked /srv/salt -> /home/oag/Projects/icecap/salt/roots.
and /srv/pillar -> /home/oag/Projects/icecap/pillar

Salt master setup is documented in readme.txt.

Configuration needed for both deployment and runtime configuration
should all come from the Salt pillar, i.e. from the first
configuration to be used.

User provides:

  pillar/passwords.sls
  salt/roots/id_rsa.pub
  local/master-ip

The upstart configs for Vagrant dev boxes need to start grid services
after the /vagrant volume is mounted. This explains how:

  http://razius.com/articles/launching-services-after-vagrant-mount/

To reload the application:

  icegridadmin --Ice.Config=/vagrant/grid/client.cfg -ux -px -e 'application update /vagrant/grid.xml'

TODO:

Make it possible to view the node logs via the gui tool

Copy the gui tool into the /vagrant shared folder and document it in
the readme file.

--

How do we generate grid.xml?

The hosts are listed in dev.sls and local.sls. If we want to generate
an app for all of them we need to load both. So what would work would
be to check what our registry IP is and then load all the platform sls
files which specify the same registry IP.

That gives us a nice list of host names. Currently we're putting
node<n> on the box whose minion id / hostname is icebox-<n>.

We've added services.yml which contains a list of service
descriptions. Adapter id's are generated according to a simple naming
convention and by default every service is added on all nodes.

--

How does this divide up now in terms of grid infrastructure v.s. the
beginnings of a specific application?

make_config.py # 
config.py
  Is used by make_config.py and grid_admin.py.
  Those are infra.

I've got a whole python-path directory to play with.
But anyway, unfortunately the python directory will be mixed if it has
to contain config.py.

application:
  pillar
  services.yml
  servers
  slice
  python

generated:
  grid
  python/icecap/config.py

infrastructure:
  local
  salt (except id_rsa.pub)
  Makefile (could easily get polluted though)
  Vagrantfile
  scripts (need separate for application)

Ok, if we keep this separation we should always be able to provide it
as a 'starter' project by emptying all the application directories
or replacing them all with a baby demo example.

To make the separation as complete as possible we should move the two
infrastructure scripts to an admin/ directory and generate a
grid_config.py file instead of config.py. The Makefile is
infrastructure now but that could easily change.

We could separate even better with symlinks but then it would be
harder to do an svn checkout on the production server.
I suppose we could check both projects out on the production
server and add links there.

Could probably eliminate local and put all vagrant stuff in the one
Vagrantfile. (Start by getting rid of install_salt.sh.)

--

Work on FakeGrid and FakeEnv.
