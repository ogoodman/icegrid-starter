This
  https://docs.vagrantup.com/v2/provisioning/salt.html
explains how to set up Vagrant to do a masterless salt provision.

Under the vagrant root directory (the one containing the Vagrantfile)
there is a salt/minion file containing only the line 
  file_client: local

I'm using 22.27.82. salt.states.pkgrepo to add the Ice repository for
Ubuntu to the host. See: 
  http://docs.saltstack.com/en/latest/ref/states/all/salt.states.pkgrepo.html
NOTE: you also need to add '- fromrepo: trusty-zeroc' to the pkg.installs 
(I believe).

Details on how to add the apt package repo came from
  http://www.zeroc.com/download.html
specifically, the section on Ubuntu 14.04 (Trusty Tahr).
See:
  http://download.zeroc.com/Ice/3.5/ubuntu/dists/trusty-zeroc/Contents-amd64
for a listing of what's in trusty-zeroc. Don't trust what ubuntu
suggests: ice35-services was not the right way to get icegridadmin.

--

Have set up a multi-machine Vagrantfile for icebox-*. See
  https://docs.vagrantup.com/v2/multi-machine/index.html

To run the salt provisioner on the vm itself:
  sudo salt-call --local state.highstate
To run it via Vagrant:
  vagrant provision icebox-<n>

To get IceGridGUI-3.5.1.jar, install icegrid somewhere
(e.g. provision one of the vagrant boxes) and copy it from
/usr/share/java. Run it as java -jar IceGridGUI-3.5.1.jar&.

The DATA_ROOT will depend on the environment. For a vagrant
development image it will be /home/vagrant/data. For production
it might be /home/icecap/data (or just /icecap/data?).

The APP_ROOT will be /vagrant for a vagrant dev image. In order to
make the slice files importable, APP_ROOT/python will have to be added
to the PYTHONPATH. It's ugly but slice2py does not seem to make it
easy to do any other way.

--

To run the demo:

  python /vagrant/scripts/demo_client.py

--

To run the registry admin tool:

  icegridadmin --Ice.Config=/vagrant/client.cfg

--

We use Vagrant to create VMs but we only go as far as
installing the salt minion.

It seems that the salt-master has a slightly annoying restriction that
it runs off a single /srv/salt. I suppose a web server has to have a
root somewhere. What I don't like is that after providing all the sls
files I will also need to document how to install them where the
salt-master can use them.

The bootstrapping process for salt minions is documented here:
  http://docs.saltstack.com/en/latest/topics/installation/ubuntu.html
It amounts to:
  sudo add-apt-repository -y ppa:saltstack/salt
  sudo apt-get update
  sudo apt-get -y install salt-minion

Then to point it at the master:
  sudo bash
  # echo '<salt-master-ip> salt' >> /etc/hosts
Alternatively we can edit /etc/salt/minion updating the line
  #master: salt
That also starts the minion. 

That forms part of the provisioning process for a new Vagrant box to
serve as an empty salt minion in the boxes directory. From that point
on the process of provisioning boxes is the normal salt master/minion
communication.

The master (whose installation process is similar, substituting 'sudo
apt-get -y install salt-master') will be contacted by the minion.
We check for the contact and accept the key proposed by the minion:
  sudo salt-key -L # lists any new keys as proposed
  sudo salt-key -a <minion-name>

Not clear to me why these commands require root access.
We can fix that:
  http://docs.saltstack.com/en/latest/ref/configuration/nonroot.html
Edited 'user' in /etc/salt/master and restarted salt-master.
(Not really sure if this is good though because of the warning in the
documentation about pam authentication issues.)
Page also says to do this (though config seems to suggest not needed):
  sudo chown -R oag /etc/salt /var/cache/salt /var/log/salt /var/run/salt
and then restart the salt-master again.

Got some "Data failed to compile === No matching sls" errors.
Went back to root again during efforts to fix all the "Data failed.."
problems. 

It seems the reason for this problem was a mismatch between the salt
versions installed on the master and minion (with the minion being
ahead due to my use of the repo there but not on the master).
I repeated the above apt-add-repository on the master and did
  sudo apt-get install salt-master --upgrade.
After that both master and client report the same version.

Symlinked /srv/salt -> /home/oag/Projects/icecap/salt/roots.
and /srv/pillar -> /home/oag/Projects/icecap/pillar

Salt master setup is documented in readme.txt.

Configuration needed for both deployment and runtime configuration
should all come from the Salt pillar, i.e. from the first
configuration to be used.

User provides:

  pillar/passwords.sls
  local/master-ip

The upstart configs for Vagrant dev boxes need to start grid services
after the /vagrant volume is mounted. This explains how:

  http://razius.com/articles/launching-services-after-vagrant-mount/

To reload the application:

  icegridadmin --Ice.Config=/vagrant/grid/client.cfg -ux -px -e 'application update /vagrant/grid.xml'

TODO:

--

Adding services to the grid

The hosts are listed in dev.sls and local.sls. If we want to generate
an app for all of them we need to load both. So what would work would
be to check what our registry IP is and then load all the platform sls
files which specify the same registry IP.

That gives us a nice list of host names. Currently we're putting
node<n> on the box whose minion id / hostname is icebox-<n>.

We've added services.yml which contains a list of service
descriptions. Adapter id's are generated according to a simple naming
convention and by default every service is added on all nodes.

--

How does this divide up now in terms of grid infrastructure v.s. the
beginnings of a specific application?

application:
  pillar
  scripts
  services.yml
  servers
  slice
  python
  doc/source/contents.txt

generated:
  grid
  python/icegrid_config.py

infrastructure:
  admin
  local
  salt (except id_rsa.pub)
  Makefile (could easily get polluted though)
  Vagrantfile
  doc/source/conf.py

Ok, if we keep this separation we should always be able to provide it
as a 'starter' project by emptying all the application directories
or replacing them all with a baby demo example.

The Makefile is infrastructure now but that could easily change.

We could separate even better with symlinks but then it would be
harder to do an svn checkout on the production server.
I suppose we could check both projects out on the production
server and add links there.

Could probably eliminate local and put all vagrant stuff in the one
Vagrantfile.

Hmm. It seems like I could do this in git by having an infrastructure
branch while having the main project in master. Or even having
infrastructure be a project from which demo and icecap both pull.
I am sure there would be problems with this.

Other things I could configure
------------------------------

Source control, bug tracking and developer wiki are commonly handled
externally. It would be possible to add Salt files for setting up
Trac though. Are there any other free options here?

The customer payments/billing system should be outsourced for startups.
(I might want to look at this for myself at some point.)

What about a public website? Well that's really as simple as
installing apache or nginx.

A development website would want some simple password protection.
It could be configured it to serve the coverage stats and source
documentation. Of course I will already do that myself.

--

What are the things that need to be documented? Can I use the
technique of the document plus the train of thought about what should
be in the document and why as a second document?

I'd like a bit more explanation of the Env philosophy. I could add
file access methods and URL retrieval. For testing a loadTestData
method would be good. Files bring in the node-id nicely.

--

Current goals?

Prepare it for GitHub. 
 * Pull code from Git repo (and set credentials to be public at GitHub).
 * To document
   * which parts used on master/minion, admin/app
   * sphinx: doc/source/contents.txt
   * nosetests: test and test-coverage targets
   * services.yml

Maybe structure the sphinx, nosetests, services.yml stuff as a
walk-through of adding a server and calling it.

For GitHub a name like icegrid_example might pick up folks who are
interested in using icegrid. At this point the "icegrid example"
Google search turns up nothing at all except a handful of links to
zeroc. The one GitHub hit turns out to be just a straight copy of the
Ice source code.

Won't merge the two Vagrantfiles because then 'vagrant up' would
make more images than we expect.

The icecap user on the local servers should be sudo enabled without a
password. Or maybe optionally so?
Of course they can administer via the vagrant user, but not so
convenient. Well it's not really needed for login at all.

It would be better if dev and local had the same number of nodes. That
means another host in local.sls and changing the wording in readme.txt
slightly.

Here is a Gotcha: if you make changes in the master and then deploy a
new 'local' box before committing those changes you may end up with
inconsistent configurations.
